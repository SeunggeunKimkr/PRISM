import torch
import typing
import transformers
import os
from typing import Union
import torch.nn.functional as F
import torchmetrics
from torch import Tensor
import math
import mauve
import lightning as L
import dataloader
import json

verbose = False

LOG2 = math.log(2)

#############################################################
# Training metrics (NLL, BPD, Perplexity): TODO: remove this or not?
#############################################################
class NLL(torchmetrics.aggregation.MeanMetric):
  pass


class BPD(NLL):
  def compute(self) -> Tensor:
    """Computes the bits per dimension.

    Returns:
      bpd
    """
    return self.mean_value / self.weight / LOG2


class Perplexity(NLL):
  def compute(self) -> Tensor:
    """Computes the Perplexity.

    Returns:
     Perplexity
    """
    return torch.exp(self.mean_value / self.weight)

def build_metric_collections(dtype=torch.float64):
    metrics = torchmetrics.MetricCollection({
        "nll": NLL(),
        "bpd": BPD(),
        "ppl": Perplexity(),
    })
    metrics.set_dtype(dtype)
    return (
        metrics.clone(prefix="train/"),
        metrics.clone(prefix="val/"),
        metrics.clone(prefix="test/"),
    )

#############################################################

class SudokuEvaluator:
  @torch.no_grad()
  def eval_sample(self, samples: Union[torch.Tensor, list[torch.Tensor]]) -> list[float]:
    if not isinstance(samples, torch.Tensor):
        samples = torch.stack(samples)
    if verbose:
        print("samples shape: ", samples.shape)
        print("samples: ", samples)
    
    results = []
    for sample in samples:
        eol_indices = (sample == 10).nonzero().flatten()
        if len(eol_indices) > 8:
            if verbose:
                print("Too many EOL tokens:", len(eol_indices))
            results.append(0.0)
            continue
        mask = (sample != 10)
        grid_values = sample[mask]
        if len(grid_values) != 81:
            if verbose:
                print("Wrong number of grid values:", len(grid_values))
            results.append(0.0)
            continue
        grid = grid_values.view(9, 9)
        valid = True
        for row in grid:
            if len(torch.unique(row)) != 9:
                valid = False
                if verbose:
                    print("invalid row: ", row)
                break
        for col in grid.t():
            if len(torch.unique(col)) != 9:
                valid = False
                if verbose:
                    print("invalid col: ", col)
                break
        for i in range(0, 9, 3):
            for j in range(0, 9, 3):
                box = grid[i:i+3, j:j+3].flatten()
                if len(torch.unique(box)) != 9:
                    valid = False
                    if verbose:
                        print("invalid box: ", box)
                    break
                    
        results.append(float(valid))
        
    return results

class TextEvaluator:
  def __init__(self, config):
    self.config = config
    self.gen_ppl_eval_model_name_or_path = config.eval.gen_ppl_eval_model_name_or_path
    self.eval_model_tokenizer = transformers.AutoTokenizer.\
      from_pretrained(self.gen_ppl_eval_model_name_or_path)
    if self.eval_model_tokenizer.pad_token is None:
      self.eval_model_tokenizer.pad_token =\
          self.eval_model_tokenizer.eos_token
      self.eval_model_tokenizer.pad_token_id =\
          self.eval_model_tokenizer.eos_token_id
    
  @torch.no_grad()
  def compute_entropy(
    self,
    sample: torch.Tensor) -> None:
    entropies = []
    for i in range(self.config.sampling.sample_batch_size):
      row = sample[i]
      counts = torch.unique(row, return_counts=True, sorted=True)[1]
      entropies.append(torch.special.entr(counts.float() / counts.sum()).sum().item())
    return sum(entropies) / len(entropies)

  @torch.no_grad()
  def eval_retokenize(self, text_samples, max_length, device):
    """Retokenizes samples for the eval model.
    
    Args:
        text_samples: List of sentences generated by the model.
    Returns:
        samples: Samples re-tokenized for the eval model
        attn_mask: Attention mask for the eval model
        eval_context_size: Size of the context for the eval model
    """
    if 'llama2' in self.gen_ppl_eval_model_name_or_path:
      tokenizer_kwargs = {
        'text_samples': text_samples,
        'return_tensors': 'pt',
        'return_token_type_ids': False,
        'return_attention_mask': True,
        'truncation': True,
        'padding': True,
        'max_length': max_length,
      }
      eval_context_size = 4096
    else:
      tokenizer_kwargs = {
        'return_tensors': 'pt',
        'return_token_type_ids': False,
        'return_attention_mask': True,
        'truncation': True,
        'padding': True,
        'max_length': max_length,
      }
      eval_context_size = 1024
    samples = self.eval_model_tokenizer(
      text_samples, ** tokenizer_kwargs)
    attn_mask = samples['attention_mask']
    samples = samples['input_ids']
    if 'llama2' not in self.gen_ppl_eval_model_name_or_path:
      attn_mask = attn_mask.to(device)
      samples = samples.to(device)      
    return samples, attn_mask, eval_context_size

  @torch.no_grad()
  def compute_generative_perplexity(
    self,
    text_samples: typing.List[str],
    gen_ppl_metric: Perplexity, 
    retokenize: bool = True,
    max_length: typing.Optional[int] = None,
    device='cpu') -> None:
    """Compute the generative perplexity of the model.

    Args:
        text_samples: List of sentences generated by the model.
    
    Returns:
        Perplexity of the generated text under a different
        pre-trained AR model (e.g., GPT2).
    """
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'
    eval_model = transformers.AutoModelForCausalLM.from_pretrained(
      self.gen_ppl_eval_model_name_or_path).eval()
    if max_length is None:
      max_length = self.config.model.length
    if 'llama2' not in self.gen_ppl_eval_model_name_or_path:
      eval_model = eval_model.to(device)
    # Re-tokenize using eval model's tokenizer
    if retokenize:
      (samples, attn_mask,
       eval_context_size) = self.eval_retokenize(
         text_samples, max_length=max_length, device=device)
    else:
      samples = text_samples
      attn_mask = torch.ones(samples.shape).to(device)
      eval_context_size = samples.shape[-1]
    batch_size = min(
      self.config.eval.perplexity_batch_size,
      samples.shape[0])
    num_batches = samples.shape[0] // batch_size
    for i in range(num_batches):
      _samples = torch.split(
        samples[i * batch_size: (i + 1) * batch_size],
        eval_context_size,
        dim=-1)
      _attn_mask = torch.split(
        attn_mask[i * batch_size: (i + 1) * batch_size],
        eval_context_size,
        dim=-1)
      for (sample_chunk, attn_mask_chunk) in zip(
        _samples, _attn_mask):
        logits = eval_model(
          sample_chunk, attention_mask=attn_mask_chunk)[0]
        logits = logits.transpose(-1, -2)
        
        nlls = F.cross_entropy(logits[..., :-1],
                               sample_chunk[..., 1:],
                               reduction='none')
        first_eos = (sample_chunk == self.eval_model_tokenizer\
                     .eos_token_id).cumsum(-1) == 1
        token_mask = (
          sample_chunk
          != self.eval_model_tokenizer.eos_token_id)
        gen_ppl_metric.update(
          nlls, first_eos[..., 1:] + token_mask[..., 1:])
  
  @L.pytorch.utilities.rank_zero_only
  @torch.no_grad()
  def compute_mauve(
    self,
    text_samples: typing.List[str],
    tokenizer) -> float:
    """Compute the MAUVE score of the model.

    Args:
        text_samples: List of sentences generated by the model.
    
    Returns:
        MAUVE score of the generated text against human references.
    """
    
    valid_loader = dataloader.get_dataloaders(
      self.config, tokenizer, valid_seed=self.config.seed, skip_train=True)[1]
    valid_ds = iter(valid_loader)
    human_references = []
    num_iters = int(self.config.sampling.num_sample_batches * 
           self.config.trainer.devices * self.config.sampling.sample_batch_size / 
           self.config.loader.eval_batch_size)
    for _ in range(num_iters):
      valid_batch = next(valid_ds)
      valid_input_ids = valid_batch['input_ids']
      human_references.extend(tokenizer.batch_decode(valid_input_ids))
    assert len(human_references) == len(text_samples)
    results = mauve.compute_mauve(p_text=human_references, q_text=text_samples, device_id=0, max_text_length=1024, verbose=False)
    mauve_score = results.mauve
    with open(self.config.sampling.generated_seqs_path, "w") as file:
      json.dump({'text_samples': text_samples, 'human_references': human_references}, file, indent=4)
    return mauve_score